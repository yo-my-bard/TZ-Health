{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obligatory imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from the newest set of facilities from HFR and dump in the data folder. Read it with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database of all the facility IDs, concatenate the variable value to URL.\n",
    "#Click Excel Export:\n",
    "#https://hfr-portal.ucchosting.co.tz/index.php?r=facilities/facilitiesWithNoGeo #no funciona\n",
    "#https://hfr-portal.ucchosting.co.tz/index.php?r=facilities/facilitiesWithGeo #no funciona\n",
    "#http://moh.go.tz/hfrportal/index.php?r=facilities/facilitiesWithNoGeo\n",
    "#http://moh.go.tz/hfrportal/index.php?r=facilities/facilitiesWithGeo\n",
    "\n",
    "url = 'http://moh.go.tz/hfrportal/index.php?r=facilities/view&facility_id='\n",
    "date = dt.datetime.now().strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This might raise an XLRDError Unsupported Format. Recommendation is to open the file and resave it with .xls extension\n",
    "#Might potentially be readable as HTML\n",
    "noGeo = pd.read_excel('./data/Facility_List_noGeo.xls', header = 2)\n",
    "withGeo = pd.read_excel('./data/Facility_List_withGeo.xls', header = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(noGeo.head())\n",
    "display(withGeo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.concat([noGeo, withGeo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()\n",
    "db.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the thing - takes approx. 15-17 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Using the IDs from the download, scrape all the websites.\n",
    "\n",
    "transposed = []\n",
    "failed = []\n",
    "print('Running through the facility numbers now...')\n",
    "counter = 0\n",
    "for e in db['Facility Number']: #e is ID, ~10310 ids total\n",
    "    if counter > 0:\n",
    "        try:\n",
    "            #When adding more rows, we shouldn't concatenate with the headers every time.\n",
    "            # We need to treat subsequent dfs, slightly differently.\n",
    "            time.sleep(3)\n",
    "            table2 = pd.read_html(url+e) #(url+ID)\n",
    "            twoTable = pd.concat(table2, ignore_index=True) #Every dataframe, must have an ID column added to it.\n",
    "            twoTable.loc[-1] = 'Facility_ID', e #Assign the ID to the last row\n",
    "            transposed.append(twoTable.transpose().drop([0])) #Add this to a list to be concat'd, doesn't have header\n",
    "            time.sleep(1)\n",
    "            counter += 1\n",
    "            \n",
    "            if counter % 100 == 0:\n",
    "                print(\"Have done:\",counter)\n",
    "        \n",
    "        except Exception as i:\n",
    "            print(i)\n",
    "            failed.append(url+e)\n",
    "            time.sleep(120)\n",
    "            continue\n",
    "    else:\n",
    "        table = pd.read_html(url+e) #(url+ID)\n",
    "        oneTable = pd.concat(table, ignore_index=True) #Every dataframe, must have an ID column added to it.\n",
    "        oneTable.loc[-1] = 'Facility_ID', e #Assign the ID to the last row\n",
    "        transposed.append(oneTable.transpose()) #Add this to a list to be concat'd\n",
    "        counter += 1\n",
    "\n",
    "print(\"Done scraping.\")\n",
    "fullFacility = pd.concat(transposed, ignore_index=True)\n",
    "facility_columns = fullFacility.loc[0, :].tolist()\n",
    "\n",
    "for e in facility_columns:\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\" \", \"_\")\n",
    "    e = e.replace(\" \", \"_\")\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\"'s\", \"\")\n",
    "\n",
    "fullFacility.columns = facility_columns\n",
    "fullFacility.drop([0], inplace=True)\n",
    "fullFacility.reset_index(drop=True)\n",
    "ordered_columns = [facility_columns[-1]]+facility_columns[:-1]\n",
    "fullFacility[ordered_columns].to_csv('./data/Health_Facilities_'+date+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Use list in transposed to list every df's df.columns\n",
    "#See what the column setup is like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF ANY FAIL, USE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If any fail, use this:\n",
    "import re\n",
    "\n",
    "transpose_fail = []\n",
    "for fail in failed:    \n",
    "    tables = pd.read_html(fail) #(url+ID) has multiple tables on page that we need to pull\n",
    "    Table = pd.concat(tables, ignore_index=True) #Every dataframe, must have an ID column added to it.\n",
    "    e = re.findall(\"[0-9\\-]+\", fail)[0]\n",
    "    Table.loc[-1] = 'Facility_ID', e #Assign the ID to the last row\n",
    "    transpose_fail.append(Table.transpose().drop([0])) #Add this to a list to be concat'd, doesn't have header\n",
    "\n",
    "fullFacility = pd.concat(transposed+transpose_fail, ignore_index=True)\n",
    "facility_columns = fullFacility.loc[0, :].tolist()\n",
    "\n",
    "for e in facility_columns:\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\" \", \"_\")\n",
    "    e = e.replace(\" \", \"_\")\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\"'s\", \"\")\n",
    "\n",
    "fullFacility.columns = facility_columns\n",
    "fullFacility.drop([0], inplace=True)\n",
    "fullFacility.reset_index(drop=True)\n",
    "ordered_columns = [facility_columns[-1]]+facility_columns[:-1]\n",
    "fullFacility[ordered_columns].to_csv('./data/Health_Facilities_'+date+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the original and scraped dataframes for a complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's merging time!\n",
    "merge_df = db[['Facility Number', 'Facility Name', 'Ward', 'Village/Street', 'Facility Type', 'Operating Status', 'Ownership', \n",
    " 'Latitude', 'Longitude']] #Taking subset of original dataframe columns that are not duplicates\n",
    "\n",
    "#Match ID column name for merging\n",
    "merge_columns = merge_df.columns.tolist()\n",
    "merge_columns[0] = 'Facility_ID'\n",
    "merge_df.columns = merge_columns\n",
    "\n",
    "combined_data = pd.merge(merge_df, fullFacility[ordered_columns], how='inner', on='Facility_ID')\n",
    "del combined_data['Geo-coordinates(Latitude,Longitude)'] #Remove duplicate and wrongly formatted coordinate column\n",
    "combined_data.to_csv('./data/Health_Facilities_'+date+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
