{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obligatory imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download from the newest set of facilities from HFR and dump in this folder. Read it with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database of all the facility IDs, concatenate the variable value to URL.\n",
    "#Click Excel Export here: http://hfrportal.ehealth.go.tz/index.php?r=facilities/facilitiesList\n",
    "\n",
    "url = 'http://hfrportal.ehealth.go.tz/index.php?r=facilities/view&facility_id='\n",
    "db = pd.read_excel('./Facility_List.xlsx', header = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the thing - takes approx. 15-17 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the IDs from the download, scrape all the websites.\n",
    "\n",
    "transposed = []\n",
    "failed = []\n",
    "print('Running through the facility numbers now...')\n",
    "counter = 0\n",
    "for e in db['Facility Number']: #e is ID, ~7950 ids total\n",
    "    if counter > 0:\n",
    "        try:\n",
    "            #When adding more rows, we shouldn't concatenate with the headers every time.\n",
    "            # We need to treat subsequent dfs, slightly differently.\n",
    "            time.sleep(3)\n",
    "            table2 = pd.read_html(url+e) #(url+ID)\n",
    "            twoTable = pd.concat(table2, ignore_index=True) #Every dataframe, must have an ID column added to it.\n",
    "            twoTable.loc[-1] = 'Facility_ID', e #Assign the ID to the last row\n",
    "            transposed.append(twoTable.transpose().drop([0])) #Add this to a list to be concat'd, doesn't have header\n",
    "            time.sleep(1)\n",
    "            counter += 1\n",
    "            \n",
    "            if counter % 100 == 0:\n",
    "                print(counter)\n",
    "        \n",
    "        except Exception as i:\n",
    "            print(i)\n",
    "            failed.append(url+e)\n",
    "            time.sleep(120)\n",
    "            continue\n",
    "    else:\n",
    "        table = pd.read_html(url+e) #(url+ID)\n",
    "        oneTable = pd.concat(table, ignore_index=True) #Every dataframe, must have an ID column added to it.\n",
    "        oneTable.loc[-1] = 'Facility_ID', e #Assign the ID to the last row\n",
    "        transposed.append(oneTable.transpose()) #Add this to a list to be concat'd\n",
    "        counter += 1\n",
    "\n",
    "fullFacility = pd.concat(transposed, ignore_index=True)\n",
    "facility_columns = fullFacility.loc[0, :].tolist()\n",
    "\n",
    "for e in facility_columns:\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\" \", \"_\")\n",
    "    e = e.replace(\" \", \"_\")\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\"'s\", \"\")\n",
    "\n",
    "fullFacility.columns = facility_columns\n",
    "fullFacility.drop([0], inplace=True)\n",
    "fullFacility.reset_index(drop=True)\n",
    "ordered_columns = [facility_columns[-1]]+facility_columns[:-1]\n",
    "fullFacility[ordered_columns].to_csv('./Health_Facilities_2018-09-28.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing concat behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Testing concat behavior\n",
    "test = ['107930-0', '111529-4', '107932-6']\n",
    "transposed = []\n",
    "for i in test:\n",
    "    table2 = pd.read_html(url+i)\n",
    "    twoTable = pd.concat(table2, ignore_index=True)\n",
    "    twoTable.loc[-1] = 'Facility_ID', 12\n",
    "    transposed.append(twoTable.transpose())\n",
    "\n",
    "twoTable\n",
    "\n",
    "test = pd.concat(transposed, ignore_index=True)\n",
    "test.drop([0], inplace = True)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IF ANY FAIL, USE THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If any fail, use this:\n",
    "import re\n",
    "\n",
    "transpose_fail = []\n",
    "for fail in failed:    \n",
    "    tables = pd.read_html(fail) #(url+ID) has multiple tables on page that we need to pull\n",
    "    Table = pd.concat(tables, ignore_index=True) #Every dataframe, must have an ID column added to it.\n",
    "    e = re.findall(\"[0-9\\-]+\", fail)[0]\n",
    "    Table.loc[-1] = 'Facility_ID', e #Assign the ID to the last row\n",
    "    transpose_fail.append(Table.transpose().drop([0])) #Add this to a list to be concat'd, doesn't have header\n",
    "\n",
    "fullFacility = pd.concat(transposed+transpose_fail, ignore_index=True)\n",
    "facility_columns = fullFacility.loc[0, :].tolist()\n",
    "\n",
    "for e in facility_columns:\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\" \", \"_\")\n",
    "    e = e.replace(\" \", \"_\")\n",
    "    facility_columns[facility_columns.index(e)] = e.replace(\"'s\", \"\")\n",
    "\n",
    "fullFacility.columns = facility_columns\n",
    "fullFacility.drop([0], inplace=True)\n",
    "fullFacility.reset_index(drop=True)\n",
    "ordered_columns = [facility_columns[-1]]+facility_columns[:-1]\n",
    "fullFacility[ordered_columns].to_csv('./Health_Facilities_2018-09-29.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the original and scraped dataframes for a complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It's merging time!\n",
    "merge_df = db[['Facility Number', 'Facility Name', 'Common Name', 'Ward', 'Village/Street', 'Facility Type', 'Operating Status', 'Ownership', \n",
    " 'Latitude', 'Longitude']] #Taking subset of original dataframe columns that are not duplicates\n",
    "\n",
    "#Match ID column name for merging\n",
    "merge_columns = merge_df.columns.tolist()\n",
    "merge_columns[0] = 'Facility_ID'\n",
    "merge_df.columns = merge_columns\n",
    "\n",
    "combined_data = pd.merge(merge_df, fullFacility[ordered_columns], how='inner', on='Facility_ID')\n",
    "del combined_data['Geo-coordinates(Latitude,Longitude)'] #Remove duplicate and wrongly formatted coordinate column\n",
    "combined_data.to_csv('./Health_Facilities_2018-09-30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
